{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff7bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, glob\n",
    "import os.path\n",
    "import ffmpeg\n",
    "from pydub import AudioSegment\n",
    "import pydub\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89487db3",
   "metadata": {},
   "source": [
    "# MP3 파일 wav 형식으로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fca7e0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MP3 파일 wav 형식으로 변환하기\n",
    "emotion_class = ['Exciting','Fear','Neutral','Relaxation','Sad','Tension']\n",
    "\n",
    "i = 0\n",
    "for emotion in emotion_class:\n",
    "    targerdir = r'Music_Video_Emotion_Dataset-master/' + emotion +'mp3'\n",
    "    files = os.listdir(targerdir)\n",
    "    for music in files:\n",
    "        file_name = targerdir+'/'+ music\n",
    "        AudioSegment.from_mp3(file_name).export(targerdir+'/'+str(i)+'.wav',format='wav')\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0050e66f",
   "metadata": {},
   "source": [
    "# Mel spectrogram 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea75c644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architectural constants.\n",
    "NUM_FRAMES = 96  # Frames in input mel-spectrogram patch.\n",
    "NUM_BANDS = 64  # Frequency bands in input mel-spectrogram patch.\n",
    "EMBEDDING_SIZE = 128  # Size of embedding layer.\n",
    "\n",
    "# Hyperparameters used in feature and example generation.\n",
    "SAMPLE_RATE = 16000\n",
    "STFT_WINDOW_LENGTH_SECONDS = 0.025\n",
    "STFT_HOP_LENGTH_SECONDS = 0.010\n",
    "NUM_MEL_BINS = NUM_BANDS\n",
    "MEL_MIN_HZ = 125\n",
    "MEL_MAX_HZ = 7500\n",
    "LOG_OFFSET = 0.01  # Offset used for stabilized log of input mel-spectrogram.\n",
    "EXAMPLE_WINDOW_SECONDS = 0.96  # Each example contains 96 10ms frames\n",
    "EXAMPLE_HOP_SECONDS = 0.96     # with zero overlap.\n",
    "\n",
    "# Parameters used for embedding postprocessing.\n",
    "PCA_EIGEN_VECTORS_NAME = 'pca_eigen_vectors'\n",
    "PCA_MEANS_NAME = 'pca_means'\n",
    "QUANTIZE_MIN_VAL = -2.0\n",
    "QUANTIZE_MAX_VAL = +2.0\n",
    "\n",
    "# Hyperparameters used in training.\n",
    "INIT_STDDEV = 0.01  # Standard deviation used to initialize weights.\n",
    "LEARNING_RATE = 1e-4  # Learning rate for the Adam optimizer.\n",
    "ADAM_EPSILON = 1e-8  # Epsilon for the Adam optimizer.\n",
    "\n",
    "# Names of ops, tensors, and features.\n",
    "INPUT_OP_NAME = 'vggish/input_features'\n",
    "INPUT_TENSOR_NAME = INPUT_OP_NAME + ':0'\n",
    "OUTPUT_OP_NAME = 'vggish/embedding'\n",
    "OUTPUT_TENSOR_NAME = OUTPUT_OP_NAME + ':0'\n",
    "AUDIO_EMBEDDING_FEATURE_NAME = 'audio_embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d0d34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def frame(data, window_length, hop_length):\n",
    "\n",
    "    num_samples = data.shape[0]\n",
    "    num_frames = 1 + int(np.floor((num_samples - window_length) / hop_length))\n",
    "    shape = (num_frames, window_length) + data.shape[1:]\n",
    "    strides = (data.strides[0] * hop_length,) + data.strides\n",
    "    \n",
    "    return np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\n",
    "\n",
    "\n",
    "def periodic_hann(window_length):\n",
    "\n",
    "    return 0.5 - (0.5 * np.cos(2 * np.pi / window_length *\n",
    "                             np.arange(window_length)))\n",
    "\n",
    "\n",
    "def stft_magnitude(signal, fft_length,\n",
    "                   hop_length=None,\n",
    "                   window_length=None):\n",
    "\n",
    "    frames = frame(signal, window_length, hop_length)\n",
    "    window = periodic_hann(window_length)\n",
    "    windowed_frames = frames * window\n",
    "    return np.abs(np.fft.rfft(windowed_frames, int(fft_length)))\n",
    "\n",
    "\n",
    "# Mel spectrum constants and functions.\n",
    "_MEL_BREAK_FREQUENCY_HERTZ = 700.0\n",
    "_MEL_HIGH_FREQUENCY_Q = 1127.0\n",
    "\n",
    "\n",
    "def hertz_to_mel(frequencies_hertz):\n",
    "\n",
    "    return _MEL_HIGH_FREQUENCY_Q * np.log(\n",
    "      1.0 + (frequencies_hertz / _MEL_BREAK_FREQUENCY_HERTZ))\n",
    "\n",
    "\n",
    "def spectrogram_to_mel_matrix(num_mel_bins=20,\n",
    "                              num_spectrogram_bins=129,\n",
    "                              audio_sample_rate=8000,\n",
    "                              lower_edge_hertz=125.0,\n",
    "                              upper_edge_hertz=3800.0):\n",
    "\n",
    "    nyquist_hertz = audio_sample_rate / 2.\n",
    "    if lower_edge_hertz < 0.0:\n",
    "        raise ValueError(\"lower_edge_hertz %.1f must be >= 0\" % lower_edge_hertz)\n",
    "    if lower_edge_hertz >= upper_edge_hertz:\n",
    "        raise ValueError(\"lower_edge_hertz %.1f >= upper_edge_hertz %.1f\" %\n",
    "                         (lower_edge_hertz, upper_edge_hertz))\n",
    "    if upper_edge_hertz > nyquist_hertz:\n",
    "        raise ValueError(\"upper_edge_hertz %.1f is greater than Nyquist %.1f\" %\n",
    "                         (upper_edge_hertz, nyquist_hertz))\n",
    "    spectrogram_bins_hertz = np.linspace(0.0, nyquist_hertz, num_spectrogram_bins)\n",
    "    spectrogram_bins_mel = hertz_to_mel(spectrogram_bins_hertz)\n",
    "\n",
    "    band_edges_mel = np.linspace(hertz_to_mel(lower_edge_hertz),\n",
    "                                   hertz_to_mel(upper_edge_hertz), num_mel_bins + 2)\n",
    "\n",
    "    mel_weights_matrix = np.empty((num_spectrogram_bins, num_mel_bins))\n",
    "    for i in range(num_mel_bins):\n",
    "        lower_edge_mel, center_mel, upper_edge_mel = band_edges_mel[i:i + 3]\n",
    "        # Calculate lower and upper slopes for every spectrogram bin.\n",
    "        # Line segments are linear in the *mel* domain, not hertz.\n",
    "        lower_slope = ((spectrogram_bins_mel - lower_edge_mel) /\n",
    "                       (center_mel - lower_edge_mel))\n",
    "        upper_slope = ((upper_edge_mel - spectrogram_bins_mel) /\n",
    "                       (upper_edge_mel - center_mel))\n",
    "        # .. then intersect them with each other and zero.\n",
    "        mel_weights_matrix[:, i] = np.maximum(0.0, np.minimum(lower_slope,\n",
    "                                                              upper_slope))\n",
    "  # HTK excludes the spectrogram DC bin; make sure it always gets a zero\n",
    "  # coefficient.\n",
    "    mel_weights_matrix[0, :] = 0.0\n",
    "    return mel_weights_matrix\n",
    "\n",
    "\n",
    "def log_mel_spectrogram(data,\n",
    "                        audio_sample_rate=8000,\n",
    "                        log_offset=0.0,\n",
    "                        window_length_secs=0.025,\n",
    "                        hop_length_secs=0.010,\n",
    "                        **kwargs):\n",
    "\n",
    "    window_length_samples = int(round(audio_sample_rate * window_length_secs))\n",
    "    hop_length_samples = int(round(audio_sample_rate * hop_length_secs))\n",
    "    fft_length = 2 ** int(np.ceil(np.log(window_length_samples) / np.log(2.0)))\n",
    "    spectrogram = stft_magnitude(\n",
    "      data,\n",
    "      fft_length=fft_length,\n",
    "      hop_length=hop_length_samples,\n",
    "      window_length=window_length_samples)\n",
    "    mel_spectrogram = np.dot(spectrogram, spectrogram_to_mel_matrix(\n",
    "      num_spectrogram_bins=spectrogram.shape[1],\n",
    "      audio_sample_rate=audio_sample_rate, **kwargs))\n",
    "    return np.log(mel_spectrogram + log_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4146aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import resampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b99bf232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sound(data, sample_rate):\n",
    "\n",
    "\n",
    "    if len(data.shape) > 1:\n",
    "        data = np.mean(data, axis=1)\n",
    "\n",
    "    if sample_rate != SAMPLE_RATE:\n",
    "        data = resampy.resample(data, sample_rate, SAMPLE_RATE)\n",
    "\n",
    "  # Compute log mel spectrogram features.\n",
    "    log_mel = log_mel_spectrogram(\n",
    "      data,\n",
    "      audio_sample_rate=SAMPLE_RATE,\n",
    "      log_offset=LOG_OFFSET,\n",
    "      window_length_secs=STFT_WINDOW_LENGTH_SECONDS,\n",
    "      hop_length_secs=STFT_HOP_LENGTH_SECONDS,\n",
    "      num_mel_bins=NUM_MEL_BINS,\n",
    "      lower_edge_hertz=MEL_MIN_HZ,\n",
    "      upper_edge_hertz=MEL_MAX_HZ)\n",
    "\n",
    "  # Frame features into examples.\n",
    "    features_sample_rate = 1.0 / STFT_HOP_LENGTH_SECONDS\n",
    "    example_window_length = int(round(\n",
    "      EXAMPLE_WINDOW_SECONDS * features_sample_rate))\n",
    "    example_hop_length = int(round(\n",
    "      EXAMPLE_HOP_SECONDS * features_sample_rate))\n",
    "    log_mel_examples = frame(\n",
    "      log_mel,\n",
    "      window_length=example_window_length,\n",
    "      hop_length=example_hop_length)\n",
    "    return log_mel_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "14dc893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "emotion_class = ['Exciting','Fear','Neutral','Relaxation','Sad','Tension']\n",
    "X_train = np.empty((0,96,64))\n",
    "X_test = np.empty((0,96,64))\n",
    "X_val = np.empty((0,96,64))\n",
    "y_train = []\n",
    "y_test = []\n",
    "y_val = []\n",
    "i = 0\n",
    "j = 0\n",
    "for emotion in emotion_class:\n",
    "    targerdir = r'Music_Video_Emotion_Dataset-master/' + emotion +'mp3'\n",
    "    files = os.listdir(targerdir)\n",
    "    for music in files:\n",
    "        scale_file = targerdir+'/'+ music\n",
    "        scale, sr = librosa.load(scale_file)\n",
    "        # mel spectrogram extraction\n",
    "        mel_spectrogram = preprocess_sound(scale,sr)\n",
    "        # zeropadding\n",
    "        if random.random() <= 0.7:\n",
    "            X_train= np.append(X_train,mel_spectrogram, axis = 0)\n",
    "            for _ in range(mel_spectrogram.shape[0]):\n",
    "                y_train.append(i)\n",
    "        elif (random.random() > 0.7) and (random.random() <= 0.85):\n",
    "            X_test= np.append(X_test,mel_spectrogram, axis = 0)\n",
    "            for _ in range(mel_spectrogram.shape[0]):\n",
    "                y_test.append(i)\n",
    "        else:\n",
    "            X_val= np.append(X_val,mel_spectrogram, axis = 0)\n",
    "            for _ in range(mel_spectrogram.shape[0]):\n",
    "                y_val.append(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6dc32555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, shuffle=True, random_state=1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b8c598e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.reshape((X_train.shape[0],96,64,1))\n",
    "X_test=X_test.reshape((X_test.shape[0],96,64,1))\n",
    "X_val=X_val.reshape((X_val.shape[0],96,64,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "19eb1da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "y_val = np_utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef74954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('music_train.npz',X=X_train,Y=y_train)\n",
    "np.savez('music_test.npz',X=X_test,Y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f297e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
